{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project: 멋진 챗봇 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK를 활용한 BLEU Score\n",
    "참고 블루 스코어 설명 https://donghwa-kim.github.io/BLEU.html\n",
    "BLEU(Bilingual Evaluation Understudy)score란 성과지표로 데이터의 X가 순서정보를 가진 단어들(문장)로 이루어져 있고, y 또한 단어들의 시리즈(문장)로 이루어진 경우에 사용되며, 번역을 하는 모델에 주로 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# project 챗봇 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "0                       12시 땡!                하루가 또 가네요.      0\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
       "...                        ...                       ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "data_dir = os.getenv('HOME')+'/aiffel/songys_chatbot/Chatbot_data-master'\n",
    "data = pd.read_csv(\"/home/aiffel-dj57/aiffel/songys_chatbot/Chatbot_data-master/ChatbotData .csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 다운로드\n",
    "#읽어 온 데이터의 질문과 답변을 각각 questions, answers 변수에 나눠서 저장하세요!\n",
    "\n",
    "df = pd.DataFrame(data) ## 데이터프레임 생성\n",
    "questions = df['Q']\n",
    "answers = df['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         12시 땡!\n",
       "1                    1지망 학교 떨어졌어\n",
       "2                   3박4일 놀러가고 싶다\n",
       "3                3박4일 정도 놀러가고 싶다\n",
       "4                        PPL 심하네\n",
       "                  ...           \n",
       "11818             훔쳐보는 것도 눈치 보임.\n",
       "11819             훔쳐보는 것도 눈치 보임.\n",
       "11820                흑기사 해주는 짝남.\n",
       "11821    힘든 연애 좋은 연애라는게 무슨 차이일까?\n",
       "11822                 힘들어서 결혼할까봐\n",
       "Name: Q, Length: 11823, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      하루가 또 가네요.\n",
       "1                       위로해 드립니다.\n",
       "2                     여행은 언제나 좋죠.\n",
       "3                     여행은 언제나 좋죠.\n",
       "4                      눈살이 찌푸려지죠.\n",
       "                   ...           \n",
       "11818          티가 나니까 눈치가 보이는 거죠!\n",
       "11819               훔쳐보는 거 티나나봐요.\n",
       "11820                      설렜겠어요.\n",
       "11821    잘 헤어질 수 있는 사이 여부인 거 같아요.\n",
       "11822          도피성 결혼은 하지 않길 바라요.\n",
       "Name: A, Length: 11823, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 정제\n",
    "#영문자의 경우, 모두 소문자로 변환합니다.\n",
    "#영문자와 한글, 숫자, 그리고 주요 특수문자를 제외하곤 정규식을 활용하여 모두 제거합니다.\n",
    "\n",
    "\n",
    "def preprocess_sentence(sentence, s_token=False, e_token=False):\n",
    "    sentence = sentence.lower().strip()\n",
    "    \n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^ㄱ-ㅣ가-힣a-zA-Z0-9?.!,]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    if s_token:\n",
    "        sentence = '<start> ' + sentence\n",
    "\n",
    "    if e_token:\n",
    "        sentence += ' <end>'\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰화\n",
    "토큰화에는 KoNLPy의 mecab 클래스를 사용합니다.\n",
    "\n",
    "아래 조건을 만족하는 build_corpus() 함수를 구현하세요!\n",
    "\n",
    "1. 소스 문장 데이터와 타겟 문장 데이터를 입력으로 받습니다.\n",
    "2. 데이터를 앞서 정의한 preprocess_sentence() 함수로 정제하고, 토큰화합니다. 토큰화는 전달받은 토크나이즈 함수를 사용합니다. 이번엔 mecab.morphs 함수를 전달하시면 됩니다.\n",
    "3. 토큰의 개수가 일정 길이 이상인 문장은 데이터에서 제외합니다.중복되는 문장은 데이터에서 제외합니다. 소스 : 타겟 쌍을 비교하지 않고 소스는 소스대로 타겟은 타겟대로 검사합니다. 중복 쌍이 흐트러지지 않도록 유의하세요!\n",
    "4. 구현한 함수를 활용하여 questions 와 answers 를 각각 que_corpus , ans_corpus 에 토큰화하여 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q_list = df.Q.str.split('_').str[0].tolist()\n",
    "#print(q_list[:10])\n",
    "#a_list = df.A.str.split('_').str[0].tolist() #리스트로 변환해줍니다. 될지는 모르겠어요 \n",
    "#print(a_list[:10])\n",
    "\n",
    "#이런 식으로 리스트 형태로 만들어 아래 코드에 하나하나 넣어줬더니 리스트 전체를 도는데 시간이 오래걸려서 에러가 생겼다. 이 방법은 아닌 것 같다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(src, tgt, tokenizer, max_len=50):\n",
    "\n",
    "    pre_src = [preprocess_sentence(x) for x in src]\n",
    "    pre_tgt = [preprocess_sentence(x) for x in tgt]\n",
    "    \n",
    "    #중복제거\n",
    "    src = []\n",
    "    tgt = []\n",
    "    \n",
    "    for so,tar in zip(pre_src, pre_tgt):\n",
    "        if so in src :\n",
    "            continue\n",
    "        if tar in tgt :\n",
    "            continue\n",
    "        src.append(so)\n",
    "        tgt.append(tar)\n",
    "    \n",
    "    que_corpus = []\n",
    "    ans_corpus = []\n",
    "\n",
    "    for so, tar in zip(src, tgt):\n",
    "        src_tokens = tokenizer.morphs(preprocess_sentence(so))\n",
    "        tgt_tokens = tokenizer.morphs(preprocess_sentence(tar))\n",
    "\n",
    "        if (len(src_tokens) > 50): continue\n",
    "        if (len(tgt_tokens) > 50): continue\n",
    "    \n",
    "        que_corpus.append(src_tokens)\n",
    "        ans_corpus.append(tgt_tokens)\n",
    "\n",
    "    return que_corpus, ans_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "que_corpus, ans_corpus = build_corpus(questions, answers, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "que [['12', '시', '땡', '!'], ['1', '지망', '학교', '떨어졌', '어'], ['3', '박', '4', '일', '놀', '러', '가', '고', '싶', '다']] + ans [['하루', '가', '또', '가', '네요', '.'], ['위로', '해', '드립니다', '.'], ['여행', '은', '언제나', '좋', '죠', '.']]\n"
     ]
    }
   ],
   "source": [
    "print('que', que_corpus[:3],'+','ans', ans_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import random\n",
    "#wv = KeyedVectors.load(\"/home/aiffel-dj57/aiffel/chatbot/ko.bin\")\n",
    "model = Word2Vec.load('/home/aiffel-dj57/aiffel/chatbot/ko.bin')\n",
    "\n",
    "#gensim을 3.8.1로 다운그레이드 한다.\n",
    "#pip install gensim==3.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lexical Substitution 구현\n",
    "def lexical_sub(sentence, word2vec):\n",
    "    import random\n",
    "\n",
    "    res = \"\"\n",
    "    toks = sentence.split()\n",
    "    \n",
    "    try:\n",
    "        _from = random.choice(toks)\n",
    "        _to = word2vec.most_similar(_from)[top][0]\n",
    "\n",
    "    except:   # 단어장에 없는 단어\n",
    "        return None\n",
    "\n",
    "    for tok in toks:\n",
    "        if tok is _from: res += _to + \" \"\n",
    "        else: res += tok + \" \"\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Augmentation된 que_corpus 와 원본 ans_corpus 가 병렬을 이루도록, \n",
    "#que_corpus 와 Augmentation된 ans_corpus 가 병렬을 이루도록 하여 전체 데이터가 원래의 3배가량으로 늘어나도록 합니다.\n",
    "#데이터 30000개!\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "new_corpus_qu = []\n",
    "new_corpus_ans = []\n",
    "\n",
    "for idx in tqdm_notebook(range(3000)):\n",
    "    old_src = tokenizer.decode_ids(src_corpus[idx])\n",
    "\n",
    "    new_src = lexical_sub(old_src, wv)\n",
    "\n",
    "    if new_src is not None: new_corpus.append(new_src)\n",
    "\n",
    "    new_corpus.append(old_src)\n",
    "\n",
    "print(new_corpus[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
