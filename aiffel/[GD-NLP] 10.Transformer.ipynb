{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#트랜스포머"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "import seaborn # Attention 시각화를 위해 필요!\n",
    "\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 정제 및 토큰화\n",
    "data_dir = os.getenv('HOME')+'/aiffel/transformer/data'\n",
    "kor_path = data_dir+\"/korean-english-park.train.ko\"\n",
    "eng_path = data_dir+\"/korean-english-park.train.en\"\n",
    "\n",
    "# 데이터 정제 및 토큰화\n",
    "def clean_corpus(kor_path, eng_path):\n",
    "    with open(kor_path, \"r\") as f: kor = f.read().splitlines()\n",
    "    with open(eng_path, \"r\") as f: eng = f.read().splitlines()\n",
    "    assert len(kor) == len(eng)\n",
    "\n",
    "    sen_idx = {}\n",
    "    cleaned_en = []\n",
    "    cleaned_ko = []\n",
    "\n",
    "    for sen1,sen2 in zip(eng,kor): \n",
    "        if sen1 not in sen_idx:\n",
    "            sen_idx[sen1] = 1\n",
    "            cleaned_en.append(sen1)\n",
    "            cleaned_ko.append(sen2)\n",
    "\n",
    "    return cleaned_ko, cleaned_en\n",
    "\n",
    "cleaned_corpus_ko, cleaned_corpus_en  = clean_corpus(kor_path, eng_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 75598\n",
      "Data Size(eng): 75598\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Size:\", len(cleaned_corpus_ko))\n",
    "print(\"Data Size(eng):\", len(cleaned_corpus_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정제된 데이터가 50개 이상의 토큰을 갖는 경우 제거\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,¿¡])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿¡]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "#bleu노드를 참고하였습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentencepiece를 활용하여 학습한 tokenizer를 생성합니다.\n",
    "import sentencepiece as spm\n",
    "def generate_tokenizer(corpus,\n",
    "                        vocab_size,\n",
    "                        lang=\"ko\",\n",
    "                        pad_id=0,\n",
    "                        bos_id=1,\n",
    "                        eos_id=2,\n",
    "                        unk_id=3,\n",
    "                        m=0):\n",
    "    \n",
    "    templates= '--input={} \\\n",
    "    --pad_id={} \\\n",
    "    --bos_id={} \\\n",
    "    --eos_id={} \\\n",
    "    --unk_id={} \\\n",
    "    --model_prefix={} \\\n",
    "    --vocab_size={} \\\n",
    "    --character_coverage={} \\\n",
    "    --model_type={}'\n",
    "\n",
    "    \n",
    "    temp_file = os.getenv('HOME')+'/aiffel/transformer/' + lang\n",
    "    with open(temp_file, 'w') as f:\n",
    "        for row in corpus:   # 이전 스텝에서 정제했던 corpus를 활용합니다.\n",
    "            f.write(str(row) + '\\n')\n",
    "\n",
    "    train_input_file = temp_file\n",
    "    #pad_id=0  #<pad> token을 0으로 설정\n",
    "    #vocab_size = 20000 # vocab 사이즈\n",
    "    #bos_id=1 #<start> token을 1으로 설정\n",
    "    #eos_id=2 #<end> token을 2으로 설정\n",
    "    #unk_id=3 #<unknown> token을 3으로 설정\n",
    "    character_coverage = 1.0 # to reduce character set \n",
    "    \n",
    "    m_type = ['unigram','bpe','char','word']\n",
    "    model_type = m_type[m]# Choose from unigram (default), bpe, char, or word\n",
    "    \n",
    "    prefix = lang + '_' + model_type + \"_spm\" # 저장될 tokenizer 모델에 붙는 이름\n",
    "    \n",
    "\n",
    "    cmd = templates.format(train_input_file,\n",
    "                    pad_id,\n",
    "                    bos_id,\n",
    "                    eos_id,\n",
    "                    unk_id,\n",
    "                    prefix,\n",
    "                    vocab_size,\n",
    "                    character_coverage,\n",
    "                    model_type)\n",
    "    \n",
    "    spm.SentencePieceTrainer.Train(cmd)\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load('ko_unigram_spm.model')\n",
    "    \n",
    "    return tokenizer\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 5000 #아래의 에러가 발생하여 낮춰주었습니다\n",
    "\n",
    "eng_corpus = []\n",
    "kor_corpus = []\n",
    "\n",
    "for k,e in zip(cleaned_corpus_ko, cleaned_corpus_en):\n",
    "    kor_corpus.append(preprocess_sentence(k))\n",
    "    eng_corpus.append(preprocess_sentence(e))\n",
    "\n",
    "ko_tokenizer = generate_tokenizer(kor_corpus, SRC_VOCAB_SIZE, \"ko\")\n",
    "en_tokenizer = generate_tokenizer(eng_corpus, TGT_VOCAB_SIZE, \"en\")\n",
    "en_tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 에러발생\n",
    "```\n",
    "RuntimeError: Internal: /sentencepiece/python/bundled/sentencepiece/src/trainer_interface.cc(579) [(trainer_spec_.vocab_size()) == (model_proto->pieces_size())] Vocabulary size too high (20000). Please set it to a value <= 5111.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75598/75598 [00:02<00:00, 29568.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm   # Process 과정을 보기 위해\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "assert len(kor_corpus) == len(eng_corpus)\n",
    "\n",
    "# 토큰의 길이가 50 이하인 문장만 남깁니다. \n",
    "for idx in tqdm(range(len(kor_corpus))):\n",
    "    temp_ko = ko_tokenizer.EncodeAsIds(kor_corpus[idx])\n",
    "    temp_en = en_tokenizer.EncodeAsIds(eng_corpus[idx])\n",
    "    if len(temp_ko) <= 50 and len(temp_en) <= 50:\n",
    "        src_corpus.append(temp_ko)\n",
    "        tgt_corpus.append(temp_en)\n",
    "\n",
    "# 패딩처리를 완료하여 학습용 데이터를 완성합니다. \n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51943\n",
      "51943\n"
     ]
    }
   ],
   "source": [
    "print(len(enc_train))\n",
    "print(len(dec_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns\n",
    "\n",
    "print(\"슝=3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer 완성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared = shared\n",
    "\n",
    "        if shared: self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "\n",
    "        logits = self.fc(dec_out)\n",
    "\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "#마스킹\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch, length = 16, 20\n",
    "src_padding = 5\n",
    "tgt_padding = 15\n",
    "\n",
    "src_pad = tf.zeros(shape=(batch, src_padding))\n",
    "tgt_pad = tf.zeros(shape=(batch, tgt_padding))\n",
    "\n",
    "sample_data = tf.ones(shape=(batch, length))\n",
    "\n",
    "sample_src = tf.concat([sample_data, src_pad], axis=-1)\n",
    "sample_tgt = tf.concat([sample_data, tgt_pad], axis=-1)\n",
    "\n",
    "enc_mask, dec_enc_mask, dec_mask = \\\n",
    "generate_masks(sample_src, sample_tgt)\n",
    "\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133)\n",
    "\n",
    "ax1.set_title('1) Encoder Mask')\n",
    "ax2.set_title('2) Encoder-Decoder Mask')\n",
    "ax3.set_title('3) Decoder Mask')\n",
    "\n",
    "ax1.imshow(enc_mask[:3, 0, 0].numpy(), cmap='Dark2')\n",
    "ax2.imshow(dec_enc_mask[0, 0].numpy(), cmap='Dark2')\n",
    "ax3.imshow(dec_mask[0, 0].numpy(), cmap='Dark2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#트랜스포머 정의\n",
    "#하이퍼파라미터\n",
    "transformer = Transformer(\n",
    "    n_layers=2, #인코더, 디코더 수 고정(노드 요청)\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=256,\n",
    "    src_vocab_size=5000,\n",
    "    tgt_vocab_size=5000,\n",
    "    pos_len=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Step 함수 정의\n",
    "\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 번역 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 시각화 함수\n",
    "\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0, \n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 함수\n",
    "\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 및 Attention 시각화 결합\n",
    "\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/812 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "in user code:\n\n    <ipython-input-24-3405e244480e>:11 train_step  *\n        predictions, enc_attns, dec_attns, dec_enc_attns =         model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n    <ipython-input-16-73ec5f30979b>:43 call  *\n        enc_in = self.embedding(self.enc_emb, enc_in)\n    <ipython-input-16-73ec5f30979b>:36 embedding  *\n        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n    /home/aiffel-dj57/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:1175 binary_op_wrapper\n        out = r_op(x)\n    /home/aiffel-dj57/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:855 __array__\n        \" a NumPy call, which is not supported\".format(self.name))\n\n    NotImplementedError: Cannot convert a symbolic Tensor (transformer/mul:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-f0fad7c24b1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                     \u001b[0mdec_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                     optimizer)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: in user code:\n\n    <ipython-input-24-3405e244480e>:11 train_step  *\n        predictions, enc_attns, dec_attns, dec_enc_attns =         model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n    <ipython-input-16-73ec5f30979b>:43 call  *\n        enc_in = self.embedding(self.enc_emb, enc_in)\n    <ipython-input-16-73ec5f30979b>:36 embedding  *\n        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n    /home/aiffel-dj57/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:1175 binary_op_wrapper\n        out = r_op(x)\n    /home/aiffel-dj57/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:855 __array__\n        \" a NumPy call, which is not supported\".format(self.name))\n\n    NotImplementedError: Cannot convert a symbolic Tensor (transformer/mul:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\n"
     ]
    }
   ],
   "source": [
    "#훈련\n",
    "from tqdm import tqdm\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "examples = [\"오바마는 대통령이다.\",\n",
    "            \"시민들은 도시 속에 산다.\",\n",
    "            \"커피는 필요 없다.\",\n",
    "            \"일곱 명의 사망자가 발생했다.\"\n",
    "]\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING:tensorflow:AutoGraph could not transform에러 => gast 설치\n",
    "\n",
    "#ValueError: Dimensions must be equal, but are 49 and 45 for '{{node transformer/decoder/decoder_layer/multi_head_attention_2/add}} = AddV2[T=DT_FLOAT](transformer/decoder/decoder_layer/multi_head_attention_2/truediv, transformer/decoder/decoder_layer/multi_head_attention_2/mul)' with input shapes: [64,8,49,49], [64,1,49,45].\n",
    "#디맨션 에러 발생.이게 왜 생겼는지 모르겠다. => 하이퍼파라미터를 수정함\n",
    "\n",
    "#NotImplementedError: Cannot convert a symbolic Tensor (transformer/mul:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\n",
    "#=> 넘파이 버전 변경 pip install numpy==1.19.5 => 해결 못 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in examples:\n",
    "    translate(example, transformer, ko_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in examples:\n",
    "    translate(example, transformer, ko_tokenizer, en_tokenizer,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 회고\n",
    "큰일났음. 맨날 에러를 해결을 못 함. 다른 할 게 많아서 일단 제출합니다. 에러만 가득한 고잉디퍼가 되어가고 있지만 언젠가 해결할 실력이 생기려니 하며......(흑흑) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
